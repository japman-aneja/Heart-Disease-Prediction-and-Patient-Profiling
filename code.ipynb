{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCLTSLZew2yS"
      },
      "outputs": [],
      "source": [
        "!pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the dataset in the notebook :"
      ],
      "metadata": {
        "id": "H6xocTaoxcZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using the code given on website to import the dataset\n",
        "\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# fetch dataset\n",
        "heart_disease = fetch_ucirepo(id=45)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = heart_disease.data.features\n",
        "y = heart_disease.data.targets"
      ],
      "metadata": {
        "id": "caV6LJ6Wxf7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK 3.1\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "EDA & Data Preprocessing"
      ],
      "metadata": {
        "id": "woT2xbSDxxT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Handle missing values through imputation. Fill the empty cells with the median of the remaining values in that particular column.\n",
        "X = X.fillna(X.median(numeric_only=True))\n",
        "\n",
        "df = X.copy()\n",
        "df['num'] = y\n",
        "\n",
        "# Exploratory Data Analysis\n",
        "print(df.info())\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Convert the num column in the target dataset to binary: 0 remains 0 (no disease), 1-4 should all be replaced with 1 (presence of disease)\n",
        "df['num'] = df['num'].apply(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "X = df.drop('num', axis=1)\n",
        "y = df['num']\n",
        "\n",
        "# Normalize all the features using the StandardScaler tool from the sklearn.preprocessing module.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)"
      ],
      "metadata": {
        "id": "LovhcgUKxyhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3.2\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Heart Disease Prediction"
      ],
      "metadata": {
        "id": "kixEbCPTzr_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing required packages\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Logistic Regression:\\n\", classification_report(y_test, y_pred_lr))\n",
        "print(\"Random Forest:\\n\", classification_report(y_test, y_pred_rf))\n",
        "print(\"Confusion Matrix - LR:\\n\", confusion_matrix(y_test, y_pred_lr))\n",
        "print(\"Confusion Matrix - RF:\\n\", confusion_matrix(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "id": "ANmn3fkrzyAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3.3\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Cholesterol Level Prediction"
      ],
      "metadata": {
        "id": "AWF1HpdV1OGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_reg = df['chol']\n",
        "X_reg = X_scaled.drop('chol', axis=1)\n",
        "\n",
        "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "reg = LinearRegression()\n",
        "reg.fit(X_train_r, y_train_r)\n",
        "print(\"R^2 Score:\", reg.score(X_test_r, y_test_r))\n",
        "\n",
        "corr = df.corr(numeric_only=True)\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "chol_corr = corr[\"chol\"].sort_values(ascending=False)\n",
        "print(chol_corr)"
      ],
      "metadata": {
        "id": "t7qLeTrB1SSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3.4\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Principal Component Analysis\n"
      ],
      "metadata": {
        "id": "1QHoCDUK1hKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(\"Shape of reduced dataset:\", X_pca.shape)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_)+1),\n",
        "         pca.explained_variance_ratio_, marker='o')\n",
        "plt.title('Explained Variance by Principal Components')\n",
        "plt.xlabel('Component #')\n",
        "plt.ylabel('Variance Ratio')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JUhrGsi41iVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3.5\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Grouping Patients based on Health Profiles"
      ],
      "metadata": {
        "id": "fTjyTttf1lM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Elbow method\n",
        "inertia = []\n",
        "K_range = range(2, 10)\n",
        "for k in K_range:\n",
        "    km = KMeans(n_clusters=k, random_state=42)\n",
        "    km.fit(X_pca)\n",
        "    inertia.append(km.inertia_)\n",
        "\n",
        "plt.plot(K_range, inertia, marker='o')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "plt.show()\n",
        "\n",
        "# Silhouette scores\n",
        "for k in K_range:\n",
        "    km = KMeans(n_clusters=k, random_state=42)\n",
        "    labels = km.fit_predict(X_pca)\n",
        "    score = silhouette_score(X_pca, labels)\n",
        "    print(f\"k={k}, Silhouette Score={score:.4f}\")\n",
        "\n",
        "# Final KMeans model\n",
        "k_optimal = 3\n",
        "kmeans = KMeans(n_clusters=k_optimal, random_state=42)\n",
        "clusters = kmeans.fit_predict(X_pca)\n",
        "\n",
        "# 2D PCA visualization\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis', s=50)\n",
        "plt.title(\"KMeans Clusters (PCA reduced)\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.colorbar(label=\"Cluster\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "b-6u9VyZ1psl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}